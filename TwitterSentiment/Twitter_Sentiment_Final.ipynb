{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user_name</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "\n",
       "         user_name                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = 'latin-1', \n",
    "                 names = ['sentiment', 'id', 'date','query', 'user_name', 'tweet'])\n",
    "\n",
    "df.loc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.drop(columns = ['id', 'date', 'query', 'user_name'], inplace = True)\n",
    "\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform special characters\n",
    "\n",
    "1) @random_user_name --> @\n",
    "\n",
    "2) http{blah blah} --> http\n",
    "\n",
    "3) www{ blah blah} --> www\n",
    "\n",
    "4) {blah blah}com --> com\n",
    "\n",
    "5) # --> hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 31.8 ms, total: 16.4 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def transform_special_characters(df, text_column):\n",
    "    '''Takes a DataFrame, parses through the text_column\n",
    "    and transfroms http:... -> http\n",
    "    www.blahblah... -> www\n",
    "    #hashtag -> hashtag\n",
    "    @user_name -> at_symbol'''\n",
    "    \n",
    "    transforms = {r'http[^\\s]+': 'http',\n",
    "                 r'\\#\\w+': 'hashtag',\n",
    "                 r' www[^\\s]+': 'www',\n",
    "                 r'\\@\\w+': 'at_symbol',\n",
    "                 r'[^\\s]+\\.com': 'com'}\n",
    "    \n",
    "    df[text_column].replace(regex = transforms, inplace = True)\n",
    "    return None\n",
    "\n",
    "transform_special_characters(df, 'tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we perform standard steps, by converting into lower case and tokenizing.\n",
    "\n",
    "1) Convert into lower case\n",
    "\n",
    "2) Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 23s, sys: 958 ms, total: 2min 24s\n",
      "Wall time: 2min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          [at_symbol, http, -, awww, ,, that, 's, a, bummer., you, shoulda, got, david, carr, of, third, day, to, do, it., ;, d]                    \n",
       "1          [is, upset, that, he, ca, n't, update, his, facebook, by, texting, it, ..., and, might, cry, as, a, result, school, today, also., blah, !]\n",
       "2          [at_symbol, i, dived, many, times, for, the, ball., managed, to, save, 50, %, the, rest, go, out, of, bounds]                             \n",
       "3          [my, whole, body, feels, itchy, and, like, its, on, fire]                                                                                 \n",
       "4          [at_symbol, no, ,, it, 's, not, behaving, at, all., i, 'm, mad., why, am, i, here, ?, because, i, ca, n't, see, you, all, over, there, .] \n",
       "                                                                             ...                                                                     \n",
       "1599995    [just, woke, up., having, no, school, is, the, best, feeling, ever]                                                                       \n",
       "1599996    [com, -, very, cool, to, hear, old, walt, interviews, !, â, «, http]                                                                     \n",
       "1599997    [are, you, ready, for, your, mojo, makeover, ?, ask, me, for, details]                                                                    \n",
       "1599998    [happy, 38th, birthday, to, my, boo, of, alll, time, !, !, !, tupac, amaru, shakur]                                                       \n",
       "1599999    [happy, hashtag, at_symbol, at_symbol, at_symbol]                                                                                         \n",
       "Name: tweet, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "def set_to_lower(df, text_column):\n",
    "    df[text_column] = df[text_column].str.lower()\n",
    "    return None\n",
    "\n",
    "def tokenize(df, text_column):\n",
    "    tok = TreebankWordTokenizer()    \n",
    "    \n",
    "    start = 0\n",
    "    end = 20000\n",
    "    num_tweets = len(df)\n",
    "    for i in range(num_tweets//end + 1):\n",
    "        df[text_column][start:end] = df[text_column][start:end].apply(lambda row: tok.tokenize(row))\n",
    "        start = end\n",
    "        if end + 20000 < num_tweets:\n",
    "            end = end + 20000\n",
    "        else:\n",
    "            end = num_tweets\n",
    "    \n",
    "    return None\n",
    "\n",
    "set_to_lower(df, 'tweet')\n",
    "tokenize(df, 'tweet')\n",
    "df['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenizing, we use the WordNetLemmatizer in order to make our vocabulary more robust. We choose not to choose a stemmer, because stemmer is quite more crude with respect to words. Let's see if this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behaving\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          [at_symbol, http, -, awww, ,, that, 's, a, bummer., you, shoulda, got, david, carr, of, third, day, to, do, it., ;, d]                   \n",
       "1          [is, upset, that, he, ca, n't, update, his, facebook, by, texting, it, ..., and, might, cry, a, a, result, school, today, also., blah, !]\n",
       "2          [at_symbol, i, dived, many, time, for, the, ball., managed, to, save, 50, %, the, rest, go, out, of, bound]                              \n",
       "3          [my, whole, body, feel, itchy, and, like, it, on, fire]                                                                                  \n",
       "4          [at_symbol, no, ,, it, 's, not, behaving, at, all., i, 'm, mad., why, am, i, here, ?, because, i, ca, n't, see, you, all, over, there, .]\n",
       "                                                                             ...                                                                    \n",
       "1599995    [just, woke, up., having, no, school, is, the, best, feeling, ever]                                                                      \n",
       "1599996    [com, -, very, cool, to, hear, old, walt, interview, !, â, «, http]                                                                     \n",
       "1599997    [are, you, ready, for, your, mojo, makeover, ?, ask, me, for, detail]                                                                    \n",
       "1599998    [happy, 38th, birthday, to, my, boo, of, alll, time, !, !, !, tupac, amaru, shakur]                                                      \n",
       "1599999    [happy, hashtag, at_symbol, at_symbol, at_symbol]                                                                                        \n",
       "Name: tweet, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(df, text_column):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for row in df[text_column]:\n",
    "        i = 0\n",
    "        n = len(row)\n",
    "        while i < n:\n",
    "            row[i] = lemmatizer.lemmatize(row[i])\n",
    "            i += 1\n",
    "    return None\n",
    "\n",
    "lemmatize(df, 'tweet')\n",
    "print(WordNetLemmatizer().lemmatize('behaving'))\n",
    "df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "\n",
    "for row in df['tweet']:\n",
    "    for word in row:\n",
    "        if word in vocabulary:\n",
    "            vocabulary[word] += 1\n",
    "        else:\n",
    "            vocabulary[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Words: 282581\n",
      "Number of words in total: 420459\n"
     ]
    }
   ],
   "source": [
    "uniques = 0\n",
    "for (key,value) in vocabulary.items():\n",
    "    if value == 1:\n",
    "        uniques += 1\n",
    "print(\"Number of unique Words:\", uniques)\n",
    "print(\"Number of words in total:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That shows that we have 282581 unique words in our vocabulary. For convenience and faster convergence, we will delete these words and replace them with UNKNOWN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420460 282581\n"
     ]
    }
   ],
   "source": [
    "unknown_words = set({})\n",
    "\n",
    "for (key, value) in vocabulary.items():\n",
    "    if value == 1:\n",
    "        unknown_words.add(key)\n",
    "vocabulary['UNKNOWN'] = len(unknown_words)\n",
    "print(len(vocabulary), len(unknown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, http, -, awww, ,, that, 's, a, bummer., you, shoulda, got, david, carr, of, third, day, to, do, it., ;, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[is, upset, that, he, ca, n't, update, his, facebook, by, texting, it, ..., and, might, cry, a, a, result, school, today, also., blah, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, i, dived, many, time, for, the, ball., managed, to, save, 50, %, the, rest, go, out, of, bound]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[my, whole, body, feel, itchy, and, like, it, on, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, no, ,, it, 's, not, behaving, at, all., i, 'm, mad., why, am, i, here, ?, because, i, ca, n't, see, you, all, over, there, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, not, the, whole, crew]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[need, a, hug]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, hey, long, time, no, see, !, yes.., rain, a, bit, ,, only, a, bit, lol, ,, i, 'm, fine, thanks, ,, how, 's, you, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, nope, they, did, n't, have, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, que, me, UNKNOWN, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>[spring, break, in, plain, city, ..., it, 's, snowing]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment  \\\n",
       "0   0           \n",
       "1   0           \n",
       "2   0           \n",
       "3   0           \n",
       "4   0           \n",
       "5   0           \n",
       "6   0           \n",
       "7   0           \n",
       "8   0           \n",
       "9   0           \n",
       "10  0           \n",
       "\n",
       "                                                                                                                                        tweet  \n",
       "0   [at_symbol, http, -, awww, ,, that, 's, a, bummer., you, shoulda, got, david, carr, of, third, day, to, do, it., ;, d]                     \n",
       "1   [is, upset, that, he, ca, n't, update, his, facebook, by, texting, it, ..., and, might, cry, a, a, result, school, today, also., blah, !]  \n",
       "2   [at_symbol, i, dived, many, time, for, the, ball., managed, to, save, 50, %, the, rest, go, out, of, bound]                                \n",
       "3   [my, whole, body, feel, itchy, and, like, it, on, fire]                                                                                    \n",
       "4   [at_symbol, no, ,, it, 's, not, behaving, at, all., i, 'm, mad., why, am, i, here, ?, because, i, ca, n't, see, you, all, over, there, .]  \n",
       "5   [at_symbol, not, the, whole, crew]                                                                                                         \n",
       "6   [need, a, hug]                                                                                                                             \n",
       "7   [at_symbol, hey, long, time, no, see, !, yes.., rain, a, bit, ,, only, a, bit, lol, ,, i, 'm, fine, thanks, ,, how, 's, you, ?]            \n",
       "8   [at_symbol, nope, they, did, n't, have, it]                                                                                                \n",
       "9   [at_symbol, que, me, UNKNOWN, ?]                                                                                                           \n",
       "10  [spring, break, in, plain, city, ..., it, 's, snowing]                                                                                     "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_with_unknown(df, text_column, vocabulary):\n",
    "    '''Takes df and text_column to access df[text_column].\n",
    "    Then parses each row, and replaces every word that has count = 1 \n",
    "    with the word UNKNOWN. (Of course we picked capitals because unknown\n",
    "    may belong in the vocabulary)'''\n",
    "    \n",
    "    for row in df[text_column]:\n",
    "        i = 0\n",
    "        n = len(row)\n",
    "        while i < n:\n",
    "            if vocabulary[row[i]] == 1:\n",
    "                row[i] = 'UNKNOWN'\n",
    "            i += 1\n",
    "    return None\n",
    "\n",
    "replace_with_unknown(df, 'tweet', vocabulary)\n",
    "df.loc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially in the above step we replaced each word that occured once with the word UNKNOWN. The reason is twofold\n",
    "\n",
    "1) We get rid of 282581 words. This of course will give us a boost in the computing performance\n",
    "\n",
    "2) A word that appears only one on the text doesn't play any role in the training process of the XGBoost classifier (Or of any classifier for that reason). The classifier cannot understand whether the context is positive and secondly even if it understands (say this word is found on the training set), then on the test set it will not play any role. Moreover, if we generalise the classifier, this unique word has a huge possibility of appearing minimal amount of time. \n",
    "\n",
    "Therefore, it makes sense to delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    at_symbol http - awww , that 's a bummer. you shoulda got david carr of third day to do it. ; d\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].str.join(' ')\n",
    "df['tweet'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1600000x99029 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11790520 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "labels = df['sentiment'].to_numpy()/4; del df['sentiment']\n",
    "X = df['tweet'].to_numpy()\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = stopwords.words('english'))\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "X_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  18 out of  18 | elapsed: 321.0min finished\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, labels, train_size = 0.75)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "parameters = {\"max_depth\":[7, 8], \"n_estimators\":[700,900,1100]}\n",
    "cv = GridSearchCV(model, parameters, scoring = 'accuracy', n_jobs=4,verbose = True)\n",
    "\n",
    "with joblib.parallel_backend('threading',n_jobs = 4):\n",
    "    cv.fit(X_train, y_train)\n",
    "\n",
    "best = cv.best_estimator_\n",
    "print('Best model: ', best)\n",
    "print('Test score: ', accuracy_score(y_test, best.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the record (just to be funny): The execution of this cell lasted about 6 hours (17:50 - 12:00). It was extremely dissapointing to notice around 23:00 that on the line 'best = cv.best_estimator_' I had actually forgotten the last _. (Now this is fixed, in case someone wants to reproduce the result)\n",
    "\n",
    "Of course this doesn't harm the script since the cv object is of interest and this would be saved anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7713816666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = cv.best_estimator_\n",
    "results = cv.cv_results_\n",
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=8,\n",
       "              min_child_weight=1, missing=None, n_estimators=1100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.771505"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, best.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3356.768337</td>\n",
       "      <td>29.678433</td>\n",
       "      <td>45.068923</td>\n",
       "      <td>7.671434</td>\n",
       "      <td>7</td>\n",
       "      <td>700</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 700}</td>\n",
       "      <td>0.760758</td>\n",
       "      <td>0.761502</td>\n",
       "      <td>0.759779</td>\n",
       "      <td>0.760680</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3942.239021</td>\n",
       "      <td>230.673356</td>\n",
       "      <td>66.583686</td>\n",
       "      <td>12.710741</td>\n",
       "      <td>7</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 900}</td>\n",
       "      <td>0.766043</td>\n",
       "      <td>0.766485</td>\n",
       "      <td>0.765167</td>\n",
       "      <td>0.765898</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4626.904127</td>\n",
       "      <td>9.105055</td>\n",
       "      <td>156.427061</td>\n",
       "      <td>53.091106</td>\n",
       "      <td>7</td>\n",
       "      <td>1100</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 1100}</td>\n",
       "      <td>0.769523</td>\n",
       "      <td>0.769707</td>\n",
       "      <td>0.768349</td>\n",
       "      <td>0.769193</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3345.248359</td>\n",
       "      <td>31.540824</td>\n",
       "      <td>44.278647</td>\n",
       "      <td>1.307694</td>\n",
       "      <td>8</td>\n",
       "      <td>700</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 700}</td>\n",
       "      <td>0.763466</td>\n",
       "      <td>0.763975</td>\n",
       "      <td>0.763229</td>\n",
       "      <td>0.763557</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4296.554097</td>\n",
       "      <td>57.746276</td>\n",
       "      <td>106.108022</td>\n",
       "      <td>36.932765</td>\n",
       "      <td>8</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 900}</td>\n",
       "      <td>0.768201</td>\n",
       "      <td>0.769192</td>\n",
       "      <td>0.767337</td>\n",
       "      <td>0.768243</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4044.082176</td>\n",
       "      <td>867.938469</td>\n",
       "      <td>47.047902</td>\n",
       "      <td>16.822773</td>\n",
       "      <td>8</td>\n",
       "      <td>1100</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 1100}</td>\n",
       "      <td>0.771843</td>\n",
       "      <td>0.771863</td>\n",
       "      <td>0.770439</td>\n",
       "      <td>0.771382</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0  3356.768337    29.678433     45.068923        7.671434         \n",
       "1  3942.239021    230.673356    66.583686        12.710741        \n",
       "2  4626.904127    9.105055      156.427061       53.091106        \n",
       "3  3345.248359    31.540824     44.278647        1.307694         \n",
       "4  4296.554097    57.746276     106.108022       36.932765        \n",
       "5  4044.082176    867.938469    47.047902        16.822773        \n",
       "\n",
       "  param_max_depth param_n_estimators                                  params  \\\n",
       "0  7               700                {'max_depth': 7, 'n_estimators': 700}    \n",
       "1  7               900                {'max_depth': 7, 'n_estimators': 900}    \n",
       "2  7               1100               {'max_depth': 7, 'n_estimators': 1100}   \n",
       "3  8               700                {'max_depth': 8, 'n_estimators': 700}    \n",
       "4  8               900                {'max_depth': 8, 'n_estimators': 900}    \n",
       "5  8               1100               {'max_depth': 8, 'n_estimators': 1100}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0  0.760758           0.761502           0.759779           0.760680          \n",
       "1  0.766043           0.766485           0.765167           0.765898          \n",
       "2  0.769523           0.769707           0.768349           0.769193          \n",
       "3  0.763466           0.763975           0.763229           0.763557          \n",
       "4  0.768201           0.769192           0.767337           0.768243          \n",
       "5  0.771843           0.771863           0.770439           0.771382          \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0  0.000706        6                \n",
       "1  0.000548        4                \n",
       "2  0.000601        2                \n",
       "3  0.000311        5                \n",
       "4  0.000758        3                \n",
       "5  0.000666        1                "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, columns = results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Conclusions:\n",
    "\n",
    "Looking at the results table we see \n",
    "\n",
    "1) The models with the biggest number of estimators scored the highest. Rank 1, 2 have 1100 estimators, rank 3,4 have 900 and rank 5,6 have 700.\n",
    "\n",
    "2) The model that scored highest was the most complex one. That suggests, that running an iteration with depth = 8 and more iterators or depth  9 plus more iterators AND playing with other hyperparameters, may give us some better results. Due to my limited computing capacity (laptop) I refrain for the time being from doing it. However, I save the model for later use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.771505"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(best, open(\"Twitter_XGBooster.pickle.dat\", \"wb\"))\n",
    "\n",
    "experimental_model = pickle.load(open(\"Twitter_XGBooster.pickle.dat\", \"rb\"))\n",
    "\n",
    "accuracy_score(experimental_model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step is to save the model using pickle. You can reuse the model by the above code snipet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
