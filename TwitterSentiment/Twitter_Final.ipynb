{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user_name</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "\n",
       "         user_name                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = 'latin-1', \n",
    "                 names = ['sentiment', 'id', 'date','query', 'user_name', 'tweet'])\n",
    "\n",
    "df.loc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.drop(columns = ['id', 'date', 'query', 'user_name'], inplace = True)\n",
    "\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform special characters\n",
    "\n",
    "1) @random_user_name --> @\n",
    "\n",
    "2) http{blah blah} --> http\n",
    "\n",
    "3) www{ blah blah} --> www\n",
    "\n",
    "4) {blah blah}com --> com\n",
    "\n",
    "5) # --> hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 s, sys: 33.7 ms, total: 15.8 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def transform_special_characters(df, text_column):\n",
    "    '''Takes a DataFrame, parses through the text_column\n",
    "    and transfroms http:... -> http\n",
    "    www.blahblah... -> www\n",
    "    #hashtag -> hashtag\n",
    "    @user_name -> at_symbol'''\n",
    "    \n",
    "    transforms = {r'http[^\\s]+': 'http',\n",
    "                 r'\\#\\w+': 'hashtag',\n",
    "                 r' www[^\\s]+': 'www',\n",
    "                 r'\\@\\w+': 'at_symbol',\n",
    "                 r'[^\\s]+\\.com': 'com'}\n",
    "    \n",
    "    df[text_column].replace(regex = transforms, inplace = True)\n",
    "    return None\n",
    "\n",
    "transform_special_characters(df, 'tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we perform standard steps, by converting into lower case and tokenizing.\n",
    "\n",
    "1) Convert into lower case\n",
    "\n",
    "2) Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimitris/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 22s, sys: 852 ms, total: 2min 23s\n",
      "Wall time: 2min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          [at_symbol, http, -, awww, ,, that, 's, a, bummer., you, shoulda, got, david, carr, of, third, day, to, do, it., ;, d]                    \n",
       "1          [is, upset, that, he, ca, n't, update, his, facebook, by, texting, it, ..., and, might, cry, as, a, result, school, today, also., blah, !]\n",
       "2          [at_symbol, i, dived, many, times, for, the, ball., managed, to, save, 50, %, the, rest, go, out, of, bounds]                             \n",
       "3          [my, whole, body, feels, itchy, and, like, its, on, fire]                                                                                 \n",
       "4          [at_symbol, no, ,, it, 's, not, behaving, at, all., i, 'm, mad., why, am, i, here, ?, because, i, ca, n't, see, you, all, over, there, .] \n",
       "                                                                             ...                                                                     \n",
       "1599995    [just, woke, up., having, no, school, is, the, best, feeling, ever]                                                                       \n",
       "1599996    [com, -, very, cool, to, hear, old, walt, interviews, !, â, «, http]                                                                     \n",
       "1599997    [are, you, ready, for, your, mojo, makeover, ?, ask, me, for, details]                                                                    \n",
       "1599998    [happy, 38th, birthday, to, my, boo, of, alll, time, !, !, !, tupac, amaru, shakur]                                                       \n",
       "1599999    [happy, hashtag, at_symbol, at_symbol, at_symbol]                                                                                         \n",
       "Name: tweet, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "def set_to_lower(df, text_column):\n",
    "    df[text_column] = df[text_column].str.lower()\n",
    "    return None\n",
    "\n",
    "def tokenize(df, text_column):\n",
    "    tok = TreebankWordTokenizer()    \n",
    "    \n",
    "    start = 0\n",
    "    end = 20000\n",
    "    num_tweets = len(df)\n",
    "    for i in range(num_tweets//end + 1):\n",
    "        df[text_column][start:end] = df[text_column][start:end].apply(lambda row: tok.tokenize(row))\n",
    "        start = end\n",
    "        if end + 20000 < num_tweets:\n",
    "            end = end + 20000\n",
    "        else:\n",
    "            end = num_tweets\n",
    "    \n",
    "    return None\n",
    "\n",
    "set_to_lower(df, 'tweet')\n",
    "tokenize(df, 'tweet')\n",
    "df['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenizing, we use the WordNetLemmatizer in order to make our vocabulary more robust. We choose not to choose a stemmer, because stemmer is quite more crude with respect to words. Let's see if this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behaving\n",
      "CPU times: user 1min 19s, sys: 59.9 ms, total: 1min 19s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          [at_symbol, http, -, awww, ,, that, 's, a, bummer., you, shoulda, got, david, carr, of, third, day, to, do, it., ;, d]                   \n",
       "1          [is, upset, that, he, ca, n't, update, his, facebook, by, texting, it, ..., and, might, cry, a, a, result, school, today, also., blah, !]\n",
       "2          [at_symbol, i, dived, many, time, for, the, ball., managed, to, save, 50, %, the, rest, go, out, of, bound]                              \n",
       "3          [my, whole, body, feel, itchy, and, like, it, on, fire]                                                                                  \n",
       "4          [at_symbol, no, ,, it, 's, not, behaving, at, all., i, 'm, mad., why, am, i, here, ?, because, i, ca, n't, see, you, all, over, there, .]\n",
       "                                                                             ...                                                                    \n",
       "1599995    [just, woke, up., having, no, school, is, the, best, feeling, ever]                                                                      \n",
       "1599996    [com, -, very, cool, to, hear, old, walt, interview, !, â, «, http]                                                                     \n",
       "1599997    [are, you, ready, for, your, mojo, makeover, ?, ask, me, for, detail]                                                                    \n",
       "1599998    [happy, 38th, birthday, to, my, boo, of, alll, time, !, !, !, tupac, amaru, shakur]                                                      \n",
       "1599999    [happy, hashtag, at_symbol, at_symbol, at_symbol]                                                                                        \n",
       "Name: tweet, Length: 1600000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(df, text_column):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for row in df[text_column]:\n",
    "        i = 0\n",
    "        n = len(row)\n",
    "        while i < n:\n",
    "            row[i] = lemmatizer.lemmatize(row[i])\n",
    "            i += 1\n",
    "    return None\n",
    "\n",
    "lemmatize(df, 'tweet')\n",
    "print(WordNetLemmatizer().lemmatize('behaving'))\n",
    "df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "\n",
    "for row in df['tweet']:\n",
    "    for word in row:\n",
    "        if word in vocabulary:\n",
    "            vocabulary[word] += 1\n",
    "        else:\n",
    "            vocabulary[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 282581\n",
      "Numer of words that appear between 2 and 5 times 84705\n",
      "Number of words in total: 420459\n"
     ]
    }
   ],
   "source": [
    "uniques = 0\n",
    "between_2_and_5 = 0\n",
    "for (key,value) in vocabulary.items():\n",
    "    if value == 1:\n",
    "        uniques += 1\n",
    "    if value >=2 and value <=5:\n",
    "        between_2_and_5 += 1\n",
    "print(\"Number of unique words:\", uniques)\n",
    "print(\"Numer of words that appear between 2 and 5 times\", between_2_and_5)\n",
    "print(\"Number of words in total:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That shows that we have 282581 unique words, and 84705 words that appear between 2 and 5 times in our vocabulary. For convenience and faster convergence, we will delete these words (More precicely, the words between 2 and 4) and replace them with UNKNOWN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420460 359507\n"
     ]
    }
   ],
   "source": [
    "unknown_words = set({})\n",
    "\n",
    "values_to_delete = set({1,2,3,4})\n",
    "for (key, value) in vocabulary.items():\n",
    "    if value in values_to_delete:\n",
    "        unknown_words.add(key)\n",
    "        \n",
    "vocabulary['UNKNOWN'] = len(unknown_words)\n",
    "print(len(vocabulary), len(unknown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, http, -, awww, ,, that, 's, a, bummer., you, shoulda, got, david, carr, of, third, day, to, do, it., ;, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[is, upset, that, he, ca, n't, update, his, facebook, by, texting, it, ..., and, might, cry, a, a, result, school, today, also., blah, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, i, UNKNOWN, many, time, for, the, ball., managed, to, save, 50, %, the, rest, go, out, of, bound]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[my, whole, body, feel, itchy, and, like, it, on, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, no, ,, it, 's, not, behaving, at, all., i, 'm, mad., why, am, i, here, ?, because, i, ca, n't, see, you, all, over, there, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, not, the, whole, crew]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[need, a, hug]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, hey, long, time, no, see, !, yes.., rain, a, bit, ,, only, a, bit, lol, ,, i, 'm, fine, thanks, ,, how, 's, you, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, nope, they, did, n't, have, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>[at_symbol, que, me, UNKNOWN, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>[spring, break, in, plain, city, ..., it, 's, snowing]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment  \\\n",
       "0   0           \n",
       "1   0           \n",
       "2   0           \n",
       "3   0           \n",
       "4   0           \n",
       "5   0           \n",
       "6   0           \n",
       "7   0           \n",
       "8   0           \n",
       "9   0           \n",
       "10  0           \n",
       "\n",
       "                                                                                                                                        tweet  \n",
       "0   [at_symbol, http, -, awww, ,, that, 's, a, bummer., you, shoulda, got, david, carr, of, third, day, to, do, it., ;, d]                     \n",
       "1   [is, upset, that, he, ca, n't, update, his, facebook, by, texting, it, ..., and, might, cry, a, a, result, school, today, also., blah, !]  \n",
       "2   [at_symbol, i, UNKNOWN, many, time, for, the, ball., managed, to, save, 50, %, the, rest, go, out, of, bound]                              \n",
       "3   [my, whole, body, feel, itchy, and, like, it, on, fire]                                                                                    \n",
       "4   [at_symbol, no, ,, it, 's, not, behaving, at, all., i, 'm, mad., why, am, i, here, ?, because, i, ca, n't, see, you, all, over, there, .]  \n",
       "5   [at_symbol, not, the, whole, crew]                                                                                                         \n",
       "6   [need, a, hug]                                                                                                                             \n",
       "7   [at_symbol, hey, long, time, no, see, !, yes.., rain, a, bit, ,, only, a, bit, lol, ,, i, 'm, fine, thanks, ,, how, 's, you, ?]            \n",
       "8   [at_symbol, nope, they, did, n't, have, it]                                                                                                \n",
       "9   [at_symbol, que, me, UNKNOWN, ?]                                                                                                           \n",
       "10  [spring, break, in, plain, city, ..., it, 's, snowing]                                                                                     "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_with_unknown(df, text_column, vocabulary):\n",
    "    '''Takes df and text_column to access df[text_column].\n",
    "    Then parses each row, and replaces every word that has count = 1 \n",
    "    with the word UNKNOWN. (Of course we picked capitals because unknown\n",
    "    may belong in the vocabulary)'''\n",
    "    \n",
    "    values_to_delete = set({1,2,3,4})\n",
    "    for row in df[text_column]:\n",
    "        i = 0\n",
    "        n = len(row)\n",
    "        while i < n:\n",
    "            if vocabulary[row[i]] in values_to_delete:\n",
    "                row[i] = 'UNKNOWN'\n",
    "            i += 1\n",
    "    return None\n",
    "\n",
    "replace_with_unknown(df, 'tweet', vocabulary)\n",
    "df.loc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially in the above step we replaced each word that occured 1-4 times with the word UNKNOWN. The reason is twofold\n",
    "\n",
    "1) We get rid of 359507 words. This of course will give us a boost in the computing performance\n",
    "\n",
    "2) A word that appears only 1-4 on the text doesn't play any role in the training process of the XGBoost classifier (Or of any classifier for that reason). The classifier cannot understand whether the context is positive and secondly even if it understands (say this word is found on the training set), then on the test set it will possibly not play any role. Moreover, if we generalise the classifier, this unique word has a huge possibility of appearing minimal amount of time. \n",
    "\n",
    "Therefore, it makes sense to delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    at_symbol http - awww , that 's a bummer. you shoulda got david carr of third day to do it. ; d\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].str.join(' ')\n",
    "df['tweet'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1600000x46683 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11726114 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "labels = df['sentiment'].to_numpy()/4; del df['sentiment']\n",
    "X = df['tweet'].to_numpy()\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = stopwords.words('english'))\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "X_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  18 out of  18 | elapsed: 395.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=9,\n",
      "              min_child_weight=1, missing=None, n_estimators=1150, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "Test score:  0.77467\n",
      "CPU times: user 1d 57min 52s, sys: 59.1 s, total: 1d 58min 51s\n",
      "Wall time: 7h 33min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, labels, train_size = 0.75)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "parameters = {\"max_depth\":[7, 8, 9], \"n_estimators\":[1050, 1150]}\n",
    "cv = GridSearchCV(model, parameters, scoring = 'accuracy', n_jobs=4,verbose = True, cv = 3)\n",
    "\n",
    "with joblib.parallel_backend('threading',n_jobs = 4):\n",
    "    cv.fit(X_train, y_train)\n",
    "\n",
    "best = cv.best_estimator_\n",
    "print('Best model: ', best)\n",
    "print('Test score: ', accuracy_score(y_test, best.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7734325"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = cv.best_estimator_\n",
    "results = cv.cv_results_\n",
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4377.219946</td>\n",
       "      <td>4.901161</td>\n",
       "      <td>116.542374</td>\n",
       "      <td>22.916282</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 1050}</td>\n",
       "      <td>0.768568</td>\n",
       "      <td>0.767745</td>\n",
       "      <td>0.767739</td>\n",
       "      <td>0.768018</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4723.354614</td>\n",
       "      <td>85.893007</td>\n",
       "      <td>171.514794</td>\n",
       "      <td>55.574327</td>\n",
       "      <td>7</td>\n",
       "      <td>1150</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 1150}</td>\n",
       "      <td>0.769881</td>\n",
       "      <td>0.769068</td>\n",
       "      <td>0.769212</td>\n",
       "      <td>0.769387</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4818.097902</td>\n",
       "      <td>61.821466</td>\n",
       "      <td>181.904716</td>\n",
       "      <td>83.768779</td>\n",
       "      <td>8</td>\n",
       "      <td>1050</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 1050}</td>\n",
       "      <td>0.770901</td>\n",
       "      <td>0.769515</td>\n",
       "      <td>0.770229</td>\n",
       "      <td>0.770215</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5417.892612</td>\n",
       "      <td>106.698690</td>\n",
       "      <td>197.580680</td>\n",
       "      <td>46.876992</td>\n",
       "      <td>8</td>\n",
       "      <td>1150</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 1150}</td>\n",
       "      <td>0.772178</td>\n",
       "      <td>0.770765</td>\n",
       "      <td>0.771397</td>\n",
       "      <td>0.771447</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5194.832294</td>\n",
       "      <td>59.543668</td>\n",
       "      <td>224.361218</td>\n",
       "      <td>65.686789</td>\n",
       "      <td>9</td>\n",
       "      <td>1050</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 1050}</td>\n",
       "      <td>0.772896</td>\n",
       "      <td>0.771602</td>\n",
       "      <td>0.772379</td>\n",
       "      <td>0.772293</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4106.084026</td>\n",
       "      <td>1029.436163</td>\n",
       "      <td>70.685420</td>\n",
       "      <td>25.201425</td>\n",
       "      <td>9</td>\n",
       "      <td>1150</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 1150}</td>\n",
       "      <td>0.774048</td>\n",
       "      <td>0.772720</td>\n",
       "      <td>0.773529</td>\n",
       "      <td>0.773432</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0  4377.219946    4.901161      116.542374       22.916282        \n",
       "1  4723.354614    85.893007     171.514794       55.574327        \n",
       "2  4818.097902    61.821466     181.904716       83.768779        \n",
       "3  5417.892612    106.698690    197.580680       46.876992        \n",
       "4  5194.832294    59.543668     224.361218       65.686789        \n",
       "5  4106.084026    1029.436163   70.685420        25.201425        \n",
       "\n",
       "  param_max_depth param_n_estimators                                  params  \\\n",
       "0  7               1050               {'max_depth': 7, 'n_estimators': 1050}   \n",
       "1  7               1150               {'max_depth': 7, 'n_estimators': 1150}   \n",
       "2  8               1050               {'max_depth': 8, 'n_estimators': 1050}   \n",
       "3  8               1150               {'max_depth': 8, 'n_estimators': 1150}   \n",
       "4  9               1050               {'max_depth': 9, 'n_estimators': 1050}   \n",
       "5  9               1150               {'max_depth': 9, 'n_estimators': 1150}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0  0.768568           0.767745           0.767739           0.768018          \n",
       "1  0.769881           0.769068           0.769212           0.769387          \n",
       "2  0.770901           0.769515           0.770229           0.770215          \n",
       "3  0.772178           0.770765           0.771397           0.771447          \n",
       "4  0.772896           0.771602           0.772379           0.772293          \n",
       "5  0.774048           0.772720           0.773529           0.773432          \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0  0.000389        6                \n",
       "1  0.000354        5                \n",
       "2  0.000566        4                \n",
       "3  0.000578        3                \n",
       "4  0.000531        2                \n",
       "5  0.000546        1                "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, columns = results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Conclusions:\n",
    "\n",
    "Looking at the results table we see \n",
    "\n",
    "1) The models with the biggest number of estimators and higher depth scored the highest. The rank is complete analogous to the complexity.\n",
    "\n",
    "2) The model that scored highest was the most complex one. That suggests, that running an iteration with depth = 8 and more iterators or depth  9 plus more iterators AND playing with other hyperparameters, may give us some better results. Due to my limited computing capacity (laptop) I refrain for the time being from doing it. However, I save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77467"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(best, open(\"XGBooster_9_1150.pickle.dat\", \"wb\"))\n",
    "\n",
    "experimental_model = pickle.load(open(\"XGBooster_9_1150.pickle.dat\", \"rb\"))\n",
    "\n",
    "accuracy_score(experimental_model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step is to save the model using pickle. You can reuse the model by the above code snipet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
